---
title: "HarvardX Professional Certificate in Data Science - MovieLens Analysis"
author: "Ferdinand Pieterse"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  pdf_document:
    toc: true
    number_sections: yes
          
        
---

```{r install load, include=FALSE}
# Install and Load required packages
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("caret", repos = "http://cran.us.r-project.org")
library(caret)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(knitr)
library(kableExtra)

```

```{r global_options, include=FALSE}

knitr::opts_chunk$set(echo = FALSE,fig.width=12, fig.height=8,
                     warning=FALSE, message=FALSE)
```

# Introduction

This project is produced for the capstone for HarvardX Professional Certificate in Data Science. It describes the basic process for developing a recommendation system.

Recommendation systems use ratings that users have given items to make specific recommendations. Companies that sell many products to many customers and permit these customers to rate their products, like Amazon, are able to collect massive datasets that can be used to predict what rating a particular user will give a specific item. Items for which a high rating is predicted for a given user are then recommended to that user when they use the system again.


## Data Set

The GroupLens research lab (https://grouplens.org) generated a database with over 20 million ratings for over 27,000 movies by more than 138,000 users. This database is referred to as the MovieLens dataset.

We use a subset of this data containing only 10 million (10M) records to analyse and generate our report.

```{r load data}

# Create edx set, validation set, and submission file

# Note: this process could take a couple of minutes

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

```
\pagebreak

## Goal

The goal of this project is to create a movie recommendation system by developing a machine learning algorithm that predicts movie ratings.

The methodology applied to evaluate the accuracy of the algorithm is the Residual Mean Square Error, or RMSE. We define ${y}_{u,i}$ as the rating for movie *i* by user *u* and denote our prediction with $\hat{y}_{u,i}$. We can interpret the RMSE similarly to a standard deviation: it is the typical error we make when predicting a movie rating and can be mathematically expressed as follows:
$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$
with N being the number of user/movie combinations and the sum occurring over all these combinations.

If this number is larger than 1, it means our typical error is larger than one star, which is not good.

The aim is for the RMSE generated by the algorhitm to be less than 0.87750 when run of a validation dataset derived from the 10M dataset mentioned above. The lower the RMSE, the better the model.




## Key Steps

* Download the 10M dataset from grouplens.org
* Partition 10M dataset into:
  * A train set which is 90% of the 10M dataset - named edx
  * A test set which is 10% of the 10M dataset - named validation
```{r}

# Validation set will be 10% of MovieLens data

set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

```
* Develop 5 models of increasing complexity and calculate the RMSE of each model
* Select the model with the lowest RMSE from the 5 developed models and recommend that model as the preferred model to predict future movie ratings.


# Analysis

## Data Cleaning

To make sure we do not include users and movies in the validation set that do not appear in the edx set, we remove these entries from the validation set.

```{r validation set completeness}

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

```

In addition we add rows removed from validation set back into edx set.
```{r add back to training set}
# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```


## Exploratory Data Analysis

The edx data set contains ```r nrow(edx)``` rows and ```r ncol(edx)``` columns.

The names of the columns are:

```{r names}
names(edx)
```
We determined there are ```r sum(is.na(edx))``` missing values in the edx dataset.

\pagebreak
The ratings given are:
```{r ratings_table}
edx %>% group_by(rating) %>% summarize(count = n())%>% 
  rename ("Rating Given" = rating) %>%
  rename (Count = count) %>%
  kable()
```


Visually we can get a better understanding of the rating distribution
```{r rating_distr, fig.align='center'}
edx %>%
	ggplot(aes(rating)) +
  geom_histogram(binwidth = 0.25, color = "black") +
  scale_x_discrete(limits = c(seq(0.5,5,0.5))) +
  scale_y_continuous(breaks = c(seq(0, 3000000, 500000))) +
  ggtitle("Ratings count by rating given") + theme(plot.title = element_text(hjust = 0.5))
```

This visualisation indicates:

+ movies are rated from 0.5 to 5.0 in 0.5 increments
+ there are no 0 ratings
+ 4 is the most popular rating
+ 0.5 is the least popular rating
+ In general, half star ratings are less common than whole star ratings

The edx dataset contains ```r n_distinct(edx$movieId)``` unique movies and ```r n_distinct(edx$userId)``` unique users.

\pagebreak
Some movies get rated more than others
```{r movies_distribution, fig.align='center'}
edx %>%
count(movieId) %>%
ggplot(aes(n)) +
geom_histogram(bins = 30, color = "black") +
scale_x_log10() +
xlab("Number of ratings") +
  ylab("Movies Count") +
ggtitle("Number of ratings per movie") + theme(plot.title = element_text(hjust = 0.5))
```

Clearly as the below table indicates certain genres receive more ratings than others
```{r genre_distribution}
edx %>% rename(Title = title) %>% group_by(Title) %>%
   summarize(count = n()) %>%
   arrange(desc(count)) %>% 
   top_n(10) %>%
   rename("Ratings Count" = count) %>%
    knitr::kable()

```

\pagebreak
Some users are more active than others at rating movies but more than 6,000 users have given between 40 and 50 ratings

```{r User_activity, fig.align='center'}
edx %>%
count(userId) %>%
ggplot(aes(n)) +
geom_histogram(bins = 30, color = "black") +
scale_x_log10() +
xlab("Number of ratings") + 
ylab("Number of users") +
ggtitle("Number of ratings given by users") + theme(plot.title = element_text(hjust = 0.5))
```

## Modelling Approach

We define 5 models of increasing complexity and use an R user defined funtion to calculate the value of the RMSE.

```{r rmse_function}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```


### Naive or base model with the same rating for all movies and users

We start with the simplest possible recommendation system: we predict the same rating for all movies regardless of user. This prediction can be found using a model based approach. A model that assumes the same rating for all movies and users with all the differences explained by random variation would look like this:
$$ Y_{u, i} = \mu + \epsilon_{u, i} $$
with $\epsilon_{u,i}$ independent errors sampled from the same distribution centered at 0 and $\mu$ the "true" rating for all movies. We know that the estimate that minimises the RMSE is the least squares estimate of $\mu$ 
and, in this case, is the average of all ratings
```{r mean}
mu <- mean(edx$rating)
```

If we predict all unknown ratings with $\mu$ we obtain the following RMSE:

```{r base_model}
# RMSE
base_rmse <- RMSE(validation$rating, mu)
base_rmse

# Load RMSE into summary dataframe
rmse_summary <- data_frame(method = "1. Base", RMSE = base_rmse) 
```
\pagebreak


### A model that takes into account an average rating by movie or a movie effect 
We know from experience that some movies are just generally rated higher than others. This intuition, that different movies are rated differently, is confirmed by data. We can augment our previous model by adding the term $b_{i}$ to represent average ranking for movie $i$:
$$ Y_{u, i} = \mu + b_{i} +\epsilon_{u, i} $$
The $b$'s are sometimes referred to as effects.

We estimate this effect by computing $\mu$ and estimating  $b_{i}$, as the average of $$Y_{u, i} - \mu$$
```{r add_movie_effect}
# Add term to represent average ranking by movie
movie_effect <- edx %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))

```

We can see that these estimates vary substantially:

```{r movie_effect_distribution, fig.align='center'}
movie_effect %>% qplot(b_i, geom ="histogram", bins = 10, data = ., color = I("black"),
ylab = "Movie Count" ) + 
  ggtitle("Count of movies with the computed b_i") + theme(plot.title = element_text(hjust = 0.5))
```

The RMSE with this model is:
```{r movie_effect_pred and rmse}
# Predictions including movie effect
predicted_ratings <- mu + validation %>%
  left_join(movie_effect, by='movieId') %>% .$b_i

# RMSE
model_1_rmse <- RMSE(predicted_ratings, validation$rating)
model_1_rmse

# Load RMSE into summary dataframe
rmse_summary <- bind_rows(rmse_summary,data_frame(method = "2. Movie Effect Model",
                                                  RMSE = model_1_rmse))

```
\pagebreak

### A model that takes into account an average rating by user as or a user effect as well as the movie effect mentioned in 2.3.2 above
Let's compute the average rating for user $u$ for those that have rated over 100 movies:

```{r avg_rating_user, fig.align='center'}
edx %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black")+ 
  ggtitle("Average rating per user") + theme(plot.title = element_text(hjust = 0.5))
```
This indicates there is substantial variability across users as well: some users are very cranky and others love every movie. This implies that a further improvement to our model may be:
$$ Y_{u, i} = \mu + b_{i} + b_{u} +\epsilon_{u, i} $$
where $b_{u}$ is a user-specific effect.
Now if a cranky user (negative $b_{u}$) rates a great movie (positive $b_{i}$), the effects counter each other and we may be able to correctly predict that this user gave this great movie a 3 rather than a 5.



```{r add_user_effect}
# Add term to represent average ranking by user
user_effect <- edx %>% 
  left_join(movie_effect, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

# Predictions including movie effect and user effect
predicted_ratings <- validation %>% 
  left_join(movie_effect, by='movieId') %>%
  left_join(user_effect, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>% .$pred
```
If we apply this model the resultant RMSE comes to:
```{r user_effect_rmse}
# RMSE
model_2_rmse <- RMSE(predicted_ratings, validation$rating)
model_2_rmse

# Load RMSE into summary dataframe
rmse_summary <- bind_rows(rmse_summary,data_frame(method = "3. Movie and user effect Model",
                                                  RMSE = model_2_rmse))
```


### A model that adds regularisation to the movie effect model mentioned in 2.3.3 above
Despite the large movie to movie variation, with the movie effect model our improvement in RMSE was only about ```r round(-model_1_rmse*100+base_rmse*100,2)```.

This is because there are obscure movies - not often rated, but with with large predictions. With just a few users, we have more uncertainty.

These are noisy estimates that we should not trust, especially when it comes to prediction. Large errors can increase our RMSE, so we would rather be conservative when unsure.

The general idea behind regularisation is to constrain the total variability of the effect sizes. Instead of minimising the least square equation, we minimise an equation that adds a penalty $\lambda$:
$$ {\frac{1}{N}\displaystyle\sum_{u,i} ({y}_{u,i}- \mu - b_{i})^{2}} + \lambda \displaystyle\sum_{i}b_{i}^{2}$$
The values of $b_{i}$ that minimise this equation are:
$$ b_{i}(\lambda) = {\frac{1}{\lambda + n_{i}}\displaystyle\sum_{u=1}^{n_{i}} ({Y}_{u,i} - \mu)} $$
where $n_{i}$is the number of ratings made for movie $i$  

Let's re-run the model with the movie effect using these regularised estimates of $b_{i}$ using $\lambda$ = 3 and compute the resultant RMSE:

```{r reg_movie_effect}
# Compute regularised estimates of movie effect
lambda <- 3
movie_reg_effect <- edx %>% 
  group_by(movieId) %>% 
  summarize(b_i = sum(rating - mu)/(n()+lambda), n_i = n()) 

# Predictions including regularised movie effect
predicted_ratings <- validation %>% 
  left_join(movie_reg_effect, by = "movieId") %>%
  mutate(pred = mu + b_i) %>% .$pred

# RMSE
model_3_rmse <- RMSE(predicted_ratings, validation$rating)
model_3_rmse 

# Load RMSE into summary dataframe
rmse_summary <- bind_rows(rmse_summary,data_frame(method = "4. Regularised movie effect Model",
                                                  RMSE = model_3_rmse))
```


### A model that adds regularisation to the user and movie effect model mentioned in 2.3.4 above

We can use regularisation for the estimate user effects as well. We are minimising:
$$ {\frac{1}{N}\displaystyle\sum_{u,i} ({y}_{u,i}- \mu - b_{i}- b_{u})^{2}} + \lambda\left( \displaystyle\sum_{i}b_{i}^{2}+ \displaystyle\sum_{i}b_{u}^{2}\right)$$
$\lambda$ is a tuning parameter. We can use cross-validation to choose it.

```{r lambdas, fig.height=6}
# Compute range of lambdas and corresponding RMSE
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  mu <- mean(edx$rating)
  
  movie_reg_effect <- edx %>% 
    group_by(movieId) %>%
    summarize(movie_reg_effect = sum(rating - mu)/(n()+l))
  
  user_reg_effect <- edx %>% 
    left_join(movie_reg_effect, by="movieId") %>%
    group_by(userId) %>%
    summarize(user_reg_effect = sum(rating - movie_reg_effect - mu)/(n()+l))
  
  predicted_ratings <- 
    validation %>% 
    left_join(movie_reg_effect, by = "movieId") %>%
    left_join(user_reg_effect, by = "userId") %>%
    mutate(pred = mu + movie_reg_effect + user_reg_effect) %>%
    .$pred
  
  return(RMSE(predicted_ratings, validation$rating))
})

qplot(lambdas,rmses, main = "RMSE for given lambda")  

```

From the above it is apparent the optimal $\lambda$ to use is ```r lambdas[which.min(rmses)]```

```{r min_lambda}
# Determine lambda that minimises RMSE
lambda <- lambdas[which.min(rmses)]
```

Which results in a RMSE of:

```{r rmse_final model}
# RMSE
model_4_rmse <- min(rmses)
model_4_rmse

# Load RMSE into summary dataframe
rmse_summary <- bind_rows(rmse_summary,data_frame(method = "5. Regularised movie and user effect Model",
                                                  RMSE = model_4_rmse))
```
\pagebreak


# Findings / Results

The table below summarises the RMSE's of the 5 models we developed:
```{r rmse_table}
rmse_summary %>% rename(Model = method) %>% kable()
```




# Conclusion

It is clear that the model taking into consideration both the movie effect and the user effect outperforms the base model as well as the model considering only the movie effect.

The regularised model taking into consideration both the movie effect and the user effect delivered the lowest RMSE and would therefore be our choice to deploy in predicting future unknown movie ratings.